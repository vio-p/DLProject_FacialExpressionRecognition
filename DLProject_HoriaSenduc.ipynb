{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLGQ9B7B0Ep",
        "outputId": "dcb94237-6d7c-401f-92bb-8001f10a87f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTkbWWWBB3V1",
        "outputId": "c9b1d765-8778-4938-a23c-04211f6da9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg16\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv"
      ],
      "metadata": {
        "id": "gEJhBIYnu35N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FER2013(Dataset):\n",
        "    def __init__(self, root, split=\"train\", transform=None):\n",
        "        assert split in [\"train\", \"test\"], \"split must be either 'train' or 'test'\"\n",
        "\n",
        "        csv_file = f\"{root}/{split}.csv\"\n",
        "\n",
        "        with open(csv_file, \"r\", newline=\"\") as file:\n",
        "            self.samples = [\n",
        "                (\n",
        "                    torch.tensor(\n",
        "                        [int(idx) for idx in row[\"pixels\"].split()], dtype=torch.uint8\n",
        "                    ).reshape(48, 48),\n",
        "                    int(row[\"emotion\"]) if \"emotion\" in row else None,\n",
        "                )\n",
        "                for row in csv.DictReader(file)\n",
        "            ]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_tensor, target = self.samples[idx]\n",
        "\n",
        "        image_tensor.unsqueeze(0).expand(3, -1, -1)\n",
        "\n",
        "        image = Image.fromarray(image_tensor.numpy())\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "hwTx5ntUu-2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "m-LBm8VpvD9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define datasets and dataloaders\n",
        "train_dataset = FER2013(\n",
        "    root=\"./data/fer2013\", split=\"train\", transform=transform\n",
        ")\n",
        "\n",
        "valid_dataset = FER2013(\n",
        "    root=\"./data/fer2013\", split=\"test\", transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "ft2N94z9vELH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the VGG16-based model\n",
        "class CustomVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=7, pretrained=True):\n",
        "        super(CustomVGG16, self).__init__()\n",
        "        self.vgg16 = vgg16(pretrained=pretrained)\n",
        "        self.vgg16.features[0] = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Flatten(),\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.Linear(1000, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg16(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4rwrvuhlvH4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomVGG16()\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "# Loss function, optimizer, and metrics\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, min_lr=1e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sfLYeakvN2B",
        "outputId": "da404c19-b343-422f-a4cd-51702458e53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomVGG16(\n",
            "  (vgg16): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): ReLU(inplace=True)\n",
            "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): ReLU(inplace=True)\n",
            "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (18): ReLU(inplace=True)\n",
            "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (20): ReLU(inplace=True)\n",
            "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (22): ReLU(inplace=True)\n",
            "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (25): ReLU(inplace=True)\n",
            "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (27): ReLU(inplace=True)\n",
            "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (29): ReLU(inplace=True)\n",
            "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): Dropout(p=0.5, inplace=False)\n",
            "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Linear(in_features=1000, out_features=32, bias=True)\n",
            "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.5, inplace=False)\n",
            "    (7): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU()\n",
            "    (10): Dropout(p=0.5, inplace=False)\n",
            "    (11): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (12): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=32, out_features=7, bias=True)\n",
            "    (15): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "24Z5kUiofF75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (3, 48, 48))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYvDrcDwfI4j",
        "outputId": "165f962d-5cf8-45e6-8328-af39fc622e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 48, 48]           1,792\n",
            "              ReLU-2           [-1, 64, 48, 48]               0\n",
            "            Conv2d-3           [-1, 64, 48, 48]          36,928\n",
            "              ReLU-4           [-1, 64, 48, 48]               0\n",
            "         MaxPool2d-5           [-1, 64, 24, 24]               0\n",
            "            Conv2d-6          [-1, 128, 24, 24]          73,856\n",
            "              ReLU-7          [-1, 128, 24, 24]               0\n",
            "            Conv2d-8          [-1, 128, 24, 24]         147,584\n",
            "              ReLU-9          [-1, 128, 24, 24]               0\n",
            "        MaxPool2d-10          [-1, 128, 12, 12]               0\n",
            "           Conv2d-11          [-1, 256, 12, 12]         295,168\n",
            "             ReLU-12          [-1, 256, 12, 12]               0\n",
            "           Conv2d-13          [-1, 256, 12, 12]         590,080\n",
            "             ReLU-14          [-1, 256, 12, 12]               0\n",
            "           Conv2d-15          [-1, 256, 12, 12]         590,080\n",
            "             ReLU-16          [-1, 256, 12, 12]               0\n",
            "        MaxPool2d-17            [-1, 256, 6, 6]               0\n",
            "           Conv2d-18            [-1, 512, 6, 6]       1,180,160\n",
            "             ReLU-19            [-1, 512, 6, 6]               0\n",
            "           Conv2d-20            [-1, 512, 6, 6]       2,359,808\n",
            "             ReLU-21            [-1, 512, 6, 6]               0\n",
            "           Conv2d-22            [-1, 512, 6, 6]       2,359,808\n",
            "             ReLU-23            [-1, 512, 6, 6]               0\n",
            "        MaxPool2d-24            [-1, 512, 3, 3]               0\n",
            "           Conv2d-25            [-1, 512, 3, 3]       2,359,808\n",
            "             ReLU-26            [-1, 512, 3, 3]               0\n",
            "           Conv2d-27            [-1, 512, 3, 3]       2,359,808\n",
            "             ReLU-28            [-1, 512, 3, 3]               0\n",
            "           Conv2d-29            [-1, 512, 3, 3]       2,359,808\n",
            "             ReLU-30            [-1, 512, 3, 3]               0\n",
            "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "           Linear-33                 [-1, 4096]     102,764,544\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                 [-1, 4096]      16,781,312\n",
            "             ReLU-37                 [-1, 4096]               0\n",
            "          Dropout-38                 [-1, 4096]               0\n",
            "           Linear-39                 [-1, 1000]       4,097,000\n",
            "              VGG-40                 [-1, 1000]               0\n",
            "          Dropout-41                 [-1, 1000]               0\n",
            "          Flatten-42                 [-1, 1000]               0\n",
            "      BatchNorm1d-43                 [-1, 1000]           2,000\n",
            "           Linear-44                   [-1, 32]          32,032\n",
            "      BatchNorm1d-45                   [-1, 32]              64\n",
            "             ReLU-46                   [-1, 32]               0\n",
            "          Dropout-47                   [-1, 32]               0\n",
            "           Linear-48                   [-1, 32]           1,056\n",
            "      BatchNorm1d-49                   [-1, 32]              64\n",
            "             ReLU-50                   [-1, 32]               0\n",
            "          Dropout-51                   [-1, 32]               0\n",
            "           Linear-52                   [-1, 32]           1,056\n",
            "      BatchNorm1d-53                   [-1, 32]              64\n",
            "             ReLU-54                   [-1, 32]               0\n",
            "           Linear-55                    [-1, 7]             231\n",
            "          Softmax-56                    [-1, 7]               0\n",
            "================================================================\n",
            "Total params: 138,394,111\n",
            "Trainable params: 138,394,111\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 10.44\n",
            "Params size (MB): 527.93\n",
            "Estimated Total Size (MB): 538.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 60\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Counter for training set\n",
        "    train_counter = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        train_counter += labels.size(0)\n",
        "        print(f'Training - Epoch [{epoch + 1}/{epochs}], Images Analyzed: {train_counter}/{len(train_dataset)}', end='\\r')\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'Training - Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}, Images Analyzed: {train_counter}/{len(train_dataset)}')\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Counter for validation set\n",
        "    valid_counter = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            valid_counter += labels.size(0)\n",
        "            print(f'Validation - Epoch [{epoch + 1}/{epochs}], Images Analyzed: {valid_counter}/{len(valid_dataset)}', end='\\r')\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'Validation - Epoch [{epoch + 1}/{epochs}], Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Images Analyzed: {valid_counter}/{len(valid_dataset)}')\n",
        "\n",
        "    scheduler.step(val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QSuyXokvTY8",
        "outputId": "957155c4-b2da-44a8-947e-7befb2fff8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "torch.save({\n",
        "    'epoch': epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'train_loss': average_loss,\n",
        "    'train_accuracy': accuracy,\n",
        "}, 'saved_model.pth')"
      ],
      "metadata": {
        "id": "RTZd4Yp5EpFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyb96cGZg3SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwAftwUfBN0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4557ad15-32e5-40bc-c54d-dc9ccc6d5781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 28.294789791107178%\n",
            "Evaluation took 3.37s\n",
            "Evaluation Metrics:\n",
            "Confusion Matrix:\n",
            "[[  75    0   96  130  260  342   55]\n",
            " [  11    0    5   17   30   43    5]\n",
            " [  75    0  124  101  239  350  135]\n",
            " [  40    0   31 1457   91  137   18]\n",
            " [  33    0   29   85  332  754   14]\n",
            " [  29    0   86   71   58   26  561]\n",
            " [  33    0   31   99  743  310   17]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "state = torch.load(\"./checkpoints/final_checkpoint_Horia.pth\")\n",
        "model.load_state_dict(state[\"model_state_dict\"])\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "accuracy = 0.0\n",
        "\n",
        "import time\n",
        "\n",
        "start_eval = time.time()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        expanded_inputs = inputs.expand(-1, 3, -1, -1)\n",
        "\n",
        "        outputs = model(expanded_inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        accuracy += (predicted == labels).sum()\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "end_eval = time.time()\n",
        "\n",
        "accuracy /= len(valid_dataset)\n",
        "print(f\"Accuracy is {accuracy.item() * 100}%\")\n",
        "print(f\"Evaluation took {end_eval - start_eval:.2f}s\")\n",
        "\n",
        "# Print metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYW8pYMqhQNF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}